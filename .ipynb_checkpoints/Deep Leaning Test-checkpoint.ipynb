{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7f91d3-3dcb-4fc3-a8c7-242fd9daa1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96400fad-6e63-49ea-b104-68c0c98855b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------\n",
    "DB_NAME = \"nba_data.db\"\n",
    "DB_URI = f\"sqlite:///{DB_NAME}\"\n",
    "engine = create_engine(DB_URI, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a706e575-4856-4a74-8d56-3f021bc4a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load Data & Sort\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_sql(\"SELECT * FROM player_game_features\", engine)\n",
    "\n",
    "# Ensure data is sorted by player and date\n",
    "df = df.sort_values(by=[\"player_id\", \"game_date\"])\n",
    "\n",
    "# Extract the season or year from 'game_date'\n",
    "df['game_year'] = pd.to_datetime(df['game_date']).dt.year\n",
    "\n",
    "# Features and target\n",
    "features = [\"player_id\", \"pts\", \"min\", \"fgm\", \"fga\", \"pts_per_min\", \"fg_pct\"]\n",
    "\n",
    "target = \"pts\"\n",
    "\n",
    "df = df.dropna(subset=features + [\"pts\"])\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"pts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4447d056-7878-43c2-abb6-0d6a95c3d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2) Helper Function: Sliding-Window Sequences\n",
    "# ------------------------------------------------------------\n",
    "def create_player_sequences_sliding_window(data, target, player_column, window_size=20):\n",
    "    \"\"\"\n",
    "    Create sequences of exactly 'window_size' timesteps for each player's timeline.\n",
    "    Each sequence is the last 'window_size' games leading up to the current game.\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for p_id, group in data.groupby(player_column):\n",
    "        # Convert to numpy\n",
    "        feats = group.drop(columns=[player_column]).values\n",
    "        targs = target[group.index].values\n",
    "        \n",
    "        # Build sequences\n",
    "        for i in range(len(feats)):\n",
    "            if i < window_size:\n",
    "                continue\n",
    "            seq = feats[i - window_size : i]\n",
    "            X_list.append(seq)\n",
    "            y_list.append(targs[i])  # Predict next game (index i)\n",
    "\n",
    "    X_arr = np.array(X_list, dtype=np.float32)\n",
    "    y_arr = np.array(y_list, dtype=np.float32)\n",
    "    return X_arr, y_arr\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Model Builder\n",
    "# ------------------------------------------------------------\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    A smaller LSTM model (cuDNN-friendly) with dropout and gradient clipping.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # If zeros are valid data, remove Masking. Otherwise, it can ignore padded zeros.\n",
    "        Masking(mask_value=0.0, input_shape=input_shape),\n",
    "        \n",
    "        # LSTM with default activation=tanh and recurrent_activation=sigmoid for cuDNN\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1)  # Regression output\n",
    "    ])\n",
    "\n",
    "    # Adam with moderate LR and gradient clipping\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25f65ee-e22f-43d5-9811-9042e1331f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (166396, 20, 6), y_train: (166396,)\n",
      "X_val:   (5537, 20, 6), y_val:   (5537,)\n",
      "Epoch 1/50\n",
      "650/650 [==============================] - 8s 10ms/step - loss: 79.0947 - mae: 6.4881 - val_loss: 47.9221 - val_mae: 5.2502 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "650/650 [==============================] - 7s 12ms/step - loss: 41.8313 - mae: 4.9492 - val_loss: 44.8154 - val_mae: 5.1257 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "650/650 [==============================] - 7s 11ms/step - loss: 40.6832 - mae: 4.8866 - val_loss: 45.7306 - val_mae: 5.1390 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "650/650 [==============================] - 7s 11ms/step - loss: 40.0866 - mae: 4.8579 - val_loss: 44.1781 - val_mae: 5.0662 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.7117 - mae: 4.8350 - val_loss: 44.6168 - val_mae: 5.0743 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.7276 - mae: 4.8315 - val_loss: 43.3098 - val_mae: 5.0362 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.6241 - mae: 4.8257 - val_loss: 43.0179 - val_mae: 5.0252 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "650/650 [==============================] - 7s 10ms/step - loss: 39.3048 - mae: 4.8103 - val_loss: 42.8688 - val_mae: 5.0505 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "650/650 [==============================] - 6s 10ms/step - loss: 39.2827 - mae: 4.8070 - val_loss: 43.7527 - val_mae: 5.0277 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.3143 - mae: 4.8086 - val_loss: 45.1683 - val_mae: 5.0889 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.2426 - mae: 4.8039 - val_loss: 42.6911 - val_mae: 5.0153 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.1241 - mae: 4.7946 - val_loss: 44.7386 - val_mae: 5.0656 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 39.1669 - mae: 4.7983 - val_loss: 43.0559 - val_mae: 5.0110 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 38.8612 - mae: 4.7836 - val_loss: 42.7352 - val_mae: 5.0101 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 38.7136 - mae: 4.7756 - val_loss: 43.1439 - val_mae: 5.0064 - lr: 5.0000e-05\n",
      "Epoch 16/50\n",
      "650/650 [==============================] - 6s 9ms/step - loss: 38.6973 - mae: 4.7685 - val_loss: 42.8224 - val_mae: 5.0041 - lr: 5.0000e-05\n",
      "22/22 [==============================] - 1s 2ms/step\n",
      "Validation (2023) MAE:  5.02\n",
      "Validation (2023) RMSE: 6.53\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Train on [2015..2022], Validate on 2023\n",
    "# ------------------------------------------------------------\n",
    "train_mask = (df['game_year'] >= 2015) & (df['game_year'] <= 2022)\n",
    "val_mask   = (df['game_year'] == 2023)\n",
    "\n",
    "train_data = df[train_mask]\n",
    "val_data   = df[val_mask]\n",
    "\n",
    "if len(train_data) == 0 or len(val_data) == 0:\n",
    "    raise ValueError(\"No data found for given train/validation years.\")\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_data[features].drop(columns=[\"player_id\"]))\n",
    "scaled_val   = scaler.transform(val_data[features].drop(columns=[\"player_id\"]))\n",
    "\n",
    "train_scaled_df = pd.DataFrame(scaled_train, index=train_data.index, columns=features[1:])\n",
    "train_scaled_df[\"player_id\"] = train_data[\"player_id\"].values\n",
    "\n",
    "val_scaled_df = pd.DataFrame(scaled_val, index=val_data.index, columns=features[1:])\n",
    "val_scaled_df[\"player_id\"] = val_data[\"player_id\"].values\n",
    "\n",
    "# Build sliding-window sequences\n",
    "window_size = 20\n",
    "X_train, y_train = create_player_sequences_sliding_window(\n",
    "    train_scaled_df, train_data[target], \"player_id\", window_size=window_size\n",
    ")\n",
    "X_val, y_val = create_player_sequences_sliding_window(\n",
    "    val_scaled_df, val_data[target], \"player_id\", window_size=window_size\n",
    ")\n",
    "\n",
    "# If there's insufficient data, the arrays might be empty\n",
    "if len(X_train) == 0 or len(X_val) == 0:\n",
    "    raise ValueError(\"Not enough data to form sliding-window sequences for train/val.\")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "\n",
    "# Convert to tf.data\n",
    "batch_size = 256\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Build & train model\n",
    "model = build_lstm_model(input_shape=(window_size, X_train.shape[2]))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on validation\n",
    "y_pred = model.predict(val_ds).flatten()\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Validation (2023) MAE:  {mae:.2f}\")\n",
    "print(f\"Validation (2023) RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a974a-efc3-4115-942a-3ff24686e9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_NBA",
   "language": "python",
   "name": "gpu_nba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
