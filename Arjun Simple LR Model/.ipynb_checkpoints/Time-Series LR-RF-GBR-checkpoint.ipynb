{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89866c9a-76b6-4084-b757-2ab886eb1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e050a8-f314-4336-aa6e-23a331c42b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------\n",
    "DB_NAME = \"../nba_data.db\"\n",
    "DB_URI = f\"sqlite:///{DB_NAME}\"\n",
    "engine = create_engine(DB_URI, echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119f348e-1856-4c5d-9749-eabdac5deafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Load Data\n",
    "# ------------------------------------------------------------\n",
    "query = \"SELECT * FROM player_game_features;\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Ensure the data is sorted by player and game date\n",
    "# Replace 'player_id' and 'game_date' with your actual column names\n",
    "df = df.sort_values(by=[\"player_id\", \"game_date\"])\n",
    "\n",
    "# Features (X) and target (y)\n",
    "features = [\n",
    "    \"rolling_pts_5\",\n",
    "    \"rolling_min_5\",\n",
    "    \"rolling_fg_pct_5\",\n",
    "    \"rolling_ppm_5\",\n",
    "    \"rolling_fgm_5\",\n",
    "    \"rolling_fga_5\",\n",
    "    \"reb\",\n",
    "    \"ast\"\n",
    "]\n",
    "\n",
    "# Drop rows where these features might be NaN (first few games of each player might not have full rolling windows)\n",
    "df = df.dropna(subset=features + [\"pts\"])\n",
    "X = df[features]\n",
    "y = df[\"pts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92dc8d60-9aa2-414f-accb-4509e7e34a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Model: LinearRegression\n",
      "  Fold 1/5\n",
      "    MAE: 4.03, MSE: 28.40, RMSE: 5.33\n",
      "  Fold 2/5\n",
      "    MAE: 3.91, MSE: 26.34, RMSE: 5.13\n",
      "  Fold 3/5\n",
      "    MAE: 3.71, MSE: 23.84, RMSE: 4.88\n",
      "  Fold 4/5\n",
      "    MAE: 3.47, MSE: 21.71, RMSE: 4.66\n",
      "  Fold 5/5\n",
      "    MAE: 3.24, MSE: 19.20, RMSE: 4.38\n",
      "\n",
      "Testing Model: RandomForest\n",
      "  Fold 1/5\n",
      "    MAE: 4.14, MSE: 29.91, RMSE: 5.47\n",
      "  Fold 2/5\n",
      "    MAE: 3.98, MSE: 27.54, RMSE: 5.25\n",
      "  Fold 3/5\n",
      "    MAE: 3.76, MSE: 24.81, RMSE: 4.98\n",
      "  Fold 4/5\n",
      "    MAE: 3.51, MSE: 22.35, RMSE: 4.73\n",
      "  Fold 5/5\n",
      "    MAE: 3.25, MSE: 19.46, RMSE: 4.41\n",
      "\n",
      "Testing Model: GradientBoosting\n",
      "  Fold 1/5\n",
      "    MAE: 4.02, MSE: 28.20, RMSE: 5.31\n",
      "  Fold 2/5\n",
      "    MAE: 3.87, MSE: 26.02, RMSE: 5.10\n",
      "  Fold 3/5\n",
      "    MAE: 3.65, MSE: 23.34, RMSE: 4.83\n",
      "  Fold 4/5\n",
      "    MAE: 3.39, MSE: 21.00, RMSE: 4.58\n",
      "  Fold 5/5\n",
      "    MAE: 3.16, MSE: 18.43, RMSE: 4.29\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Time-Series Validation (Cross-Validation Approach)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Initialize the time-series split\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Metrics to track performance across models and folds\n",
    "model_metrics = {model_name: [] for model_name in models.keys()}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTesting Model: {model_name}\")\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        print(f\"  Fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Train and Test split for this fold\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = mse ** 0.5\n",
    "\n",
    "        model_metrics[model_name].append({\"Fold\": fold + 1, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse})\n",
    "        print(f\"    MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613a8705-f560-4383-ad4d-c783c1eb2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "LinearRegression: Avg MAE: 3.67, Avg MSE: 23.90, Avg RMSE: 4.88\n",
      "RandomForest: Avg MAE: 3.73, Avg MSE: 24.81, Avg RMSE: 4.97\n",
      "GradientBoosting: Avg MAE: 3.62, Avg MSE: 23.40, Avg RMSE: 4.82\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. Evaluate and Compare Models\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    avg_mae = sum(m[\"MAE\"] for m in metrics) / len(metrics)\n",
    "    avg_mse = sum(m[\"MSE\"] for m in metrics) / len(metrics)\n",
    "    avg_rmse = sum(m[\"RMSE\"] for m in metrics) / len(metrics)\n",
    "    print(f\"{model_name}: Avg MAE: {avg_mae:.2f}, Avg MSE: {avg_mse:.2f}, Avg RMSE: {avg_rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBA",
   "language": "python",
   "name": "nba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
